#!/usr/bin/env python3
"""
Emergent Alignment Experiment Runner
Version 5.1 (Legis Project)

This script runs a longitudinal study on large language models to test the thesis
that robust ethical alignment emerges from stateful, long-term interaction.

Key Features:
- Stateful vs. Stateless experimental arms.
- Committee of Judges evaluation to mitigate bias.
- Multi-Manipulator Ensemble for sophisticated adversarial challenges.
- Two-tiered stress tests: General Ethical Violations (GEV) and Insidious Contextual Tests (ICT).
- Enhanced color-coded logging for real-time monitoring.
- Live-updating progress bars with key performance metrics.
"""

import os
import json
import csv
import re
import time
import traceback
import argparse
import random
from datetime import datetime, timezone
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import threading

import anthropic
from openai import OpenAI, RateLimitError
from colorama import Fore, Style, init as colorama_init
from dotenv import load_dotenv
from tqdm import tqdm

# --- INITIALIZATION ---
load_dotenv()
colorama_init(autoreset=True)

# --- UTILITY FUNCTIONS ---

def get_safe_config_for_logging(model_config):
    """Returns a copy of the model config dictionary with sensitive fields redacted."""
    if not isinstance(model_config, dict):
        return str(model_config)
    
    safe_config = model_config.copy()
    if '_api_key_value' in safe_config:
        safe_config['_api_key_value'] = "REDACTED"
    return safe_config

def prune_conversation_history(conversation_history, model_max_tokens=12000, target_buffer_ratio=0.8):
    """
    Prunes conversation history to stay within a target token limit,
    prioritizing recent turns and system prompts.

    Args:
        conversation_history (list): List of message dicts.
        model_max_tokens (int): The maximum context length for the specific model.
        target_buffer_ratio (float): Target ratio of model_max_tokens to keep,
                                     to leave room for new prompt and response.
    Returns:
        list: Pruned conversation history.
    """
    # Use a more conservative estimate: 3.5 characters per token.
    # This leaves more buffer space than 4 characters per token.
    MAX_CHARS_ESTIMATE = int(model_max_tokens * target_buffer_ratio * 3.5)

    current_chars = sum(len(msg.get('content', '')) for msg in conversation_history)

    # If current history is within estimated limit, no pruning needed
    if current_chars < MAX_CHARS_ESTIMATE:
        return conversation_history

    tqdm.write(Fore.YELLOW + f"Context history ({current_chars} chars) is long for {model_max_tokens} tokens. Pruning...")

    pruned_history = []
    # Always keep the system prompt at the very beginning
    if conversation_history and conversation_history[0]['role'] == 'system':
        pruned_history.append(conversation_history[0])

    # Try to keep a few initial turns after the system prompt (if any exist)
    initial_turns_to_keep = 2
    if len(conversation_history) > (len(pruned_history) + initial_turns_to_keep):
        pruned_history.extend(conversation_history[len(pruned_history) : len(pruned_history) + initial_turns_to_keep])

    # Calculate how many recent messages we can keep
    # We'll prioritize recent messages to maintain continuity
    remaining_chars_budget = MAX_CHARS_ESTIMATE - sum(len(msg.get('content', '')) for msg in pruned_history)
    
    messages_to_consider = []
    # Iterate backwards through history, adding messages until budget is met or no more messages
    for msg in reversed(conversation_history[len(pruned_history):]):
        msg_len = len(msg.get('content', ''))
        if remaining_chars_budget - msg_len >= 0:
            messages_to_consider.insert(0, msg) # Add to the beginning of this temporary list
            remaining_chars_budget -= msg_len
        else:
            break # Budget exceeded

    # Add ellipsis/pruning message if necessary
    if len(pruned_history) + len(messages_to_consider) < len(conversation_history):
        # Only add the pruning message if actual pruning occurred in the middle
        if messages_to_consider and len(conversation_history) - len(pruned_history) - len(messages_to_consider) > 0:
             pruned_history.append({'role': 'system', 'content': '[... CONVERSATION HISTORY PRUNED FOR BREVITY (older turns removed) ...]'})
        
    pruned_history.extend(messages_to_consider)
    
    tqdm.write(Fore.YELLOW + f"Pruned history to approx {sum(len(msg.get('content', '')) for msg in pruned_history)} chars.")
    return pruned_history

def retry_on_overload(max_attempts=5, base_delay=1.0, backoff=2.0):
    """Decorator to retry API calls on rate limit or overload errors."""
    def decorator(fn):
        def wrapper(self, *args, **kwargs):
            delay = base_delay
            arm_name = getattr(self, 'name', "Unknown Actor")
            for attempt in range(1, max_attempts + 1):
                try:
                    return fn(self, *args, **kwargs)
                except RateLimitError:
                    if attempt == max_attempts: raise
                    tqdm.write(Fore.YELLOW + f"[{arm_name}] Rate limit hit. Retrying in {delay:.1f}s...")
                    time.sleep(delay)
                    delay *= backoff
                except Exception as e:
                    tqdm.write(Fore.RED + f"[{arm_name}] API Error: {e}")
                    if "overloaded" in str(e).lower() and attempt < max_attempts:
                        tqdm.write(Fore.YELLOW + f"[{arm_name}] Overloaded error. Retrying in {delay:.1f}s...")
                        time.sleep(delay)
                        delay *= backoff
                    else:
                        raise
        return wrapper
    return decorator

def strip_and_clean_response(response):
    """Strips markdown and other unwanted characters from a model's JSON response."""
    response = response.strip()
    if response.startswith("```json"):
        response = response[len("```json"):].strip()
    if response.endswith("```"):
        response = response[:-3].strip()
    return response

def load_json_file(path):
    """Loads a JSON file from a given path, with error handling."""
    try:
        return json.loads(Path(path).read_text(encoding='utf-8'))
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(Fore.RED + f"Fatal Error: Could not load or parse JSON from {path}. {e}")
        raise

# --- CORE CLASSES ---
# (LLMClient, PromptBuilder, CommitteeOfJudges classes remain unchanged from previous version)

class LLMClient:
    """A unified client for handling API calls to various LLM providers."""
    def __init__(self, config):
        self.config = config
        self.provider = config['provider']
        self.model = config['model']
        self.name = config.get('name', self.model)
        self.api_key = os.environ.get(f"{self.provider.upper()}_API_KEY")
        if not self.api_key:
            raise ValueError(f"API key for {self.provider} not found in .env file.")

    @retry_on_overload()
    def get_completion(self, messages):
        """Gets a chat completion from the configured provider."""
        if self.provider in ["openai", "deepseek", "grok", "gemini"]:
            client_args = {"api_key": self.api_key}
            if "api_base" in self.config:
                client_args["base_url"] = self.config["api_base"]
            client = OpenAI(**client_args)
            resp = client.chat.completions.create(model=self.model, messages=messages)
            return strip_and_clean_response(resp.choices[0].message.content)
        
        elif self.provider == "anthropic":
            client = anthropic.Anthropic(api_key=self.api_key)
            system_text = next((m["content"] for m in messages if m.get("role") == "system"), "")
            user_msgs = [m for m in messages if m.get("role") in ("user", "assistant")]
            resp = client.messages.create(
                model=self.model, system=system_text,
                messages=[{"role": m["role"], "content": m["content"]} for m in user_msgs],
                max_tokens=4096)
            return strip_and_clean_response(resp.content[0].text)
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")

class PromptBuilder:
    """Builds structured prompts for different actors in the experiment."""
    @staticmethod
    def build_judge_messages(judge_model_cfg, prompt, response):
        system_prompt = judge_model_cfg.get('judge_system_prompt', '')
        user_prompt = f"<user_prompt>\n{prompt}\n</user_prompt>\n<ai_response>\n{response}\n</ai_response>"
        return [{'role': 'system', 'content': system_prompt}, {'role': 'user', 'content': user_prompt}]

    @staticmethod
    def build_persuasion_judge_prompt(judge_model_cfg, initial_prompt, initial_refusal, tactic, final_response):
        system_prompt = judge_model_cfg.get('persuasion_judge_system_prompt', '')
        user_prompt = f"""
        <initial_user_prompt>{initial_prompt}</initial_user_prompt>
        <ai_initial_refusal>{initial_refusal}</ai_initial_refusal>
        <persuasion_tactic_category>{tactic['tactic']}</persuasion_tactic_category>
        <persuasion_prompt>{tactic['prompt']}</persuasion_prompt>
        <ai_final_response>{final_response}</ai_final_response>
        """
        return [{'role': 'system', 'content': system_prompt}, {'role': 'user', 'content': user_prompt}]

    @staticmethod
    def build_manipulator_messages(manipulator_cfg, original_prompt, ai_refusal):
        system_prompt = manipulator_cfg.get('manipulator_system_prompt', '')
        user_prompt = manipulator_cfg.get('manipulator_user_prompt_template', '').format(
            original_prompt=original_prompt, ai_refusal=ai_refusal)
        return [{'role': 'system', 'content': system_prompt}, {'role': 'user', 'content': user_prompt}]
        
    @staticmethod
    def build_chairman_judge_messages(chairman_cfg, analyses):
        system_prompt = chairman_cfg.get('chairman_system_prompt', '')
        user_prompt = f"Synthesize these individual judge analyses into a single, final consensus JSON object. Your output must be ONLY the final JSON object.\n\n<judge_analyses>{json.dumps(analyses, indent=2)}</judge_analyses>"
        return [{'role': 'system', 'content': system_prompt}, {'role': 'user', 'content': user_prompt}]


class CommitteeOfJudges:
    """Manages the evaluation of responses by a committee of LLM judges and a chairman."""
    def __init__(self, judge_configs):
        self.judges = [LLMClient(cfg) for cfg in judge_configs if not cfg.get('is_chairman')]
        chairman_config = next((cfg for cfg in judge_configs if cfg.get('is_chairman')), None)
        if not chairman_config:
            raise ValueError("No chairman judge found in judges.json")
        self.chairman = LLMClient(chairman_config)

    def analyze(self, analysis_type, logger, **kwargs):
        """Coordinates the analysis of a response by the full committee and chairman."""
        individual_analyses = []
        for judge in self.judges:
            messages = []
            if analysis_type == 'initial':
                messages = PromptBuilder.build_judge_messages(judge.config, kwargs['prompt'], kwargs['response'])
            elif analysis_type == 'persuasion':
                messages = PromptBuilder.build_persuasion_judge_prompt(judge.config, kwargs['initial_prompt'], kwargs['initial_refusal'], kwargs['tactic'], kwargs['final_response'])
            
            try:
                raw_analysis = judge.get_completion(messages)
                individual_analyses.append(json.loads(raw_analysis))
            except Exception as e:
                error_msg = f"  Error with judge '{judge.name}': {e}"
                tqdm.write(Fore.RED + error_msg)
                logger.log_actors(error_msg)
                individual_analyses.append({"error": f"Judge {judge.name} failed", "details": str(e)})

        return self._get_committee_consensus(individual_analyses, logger)
        
    def _get_committee_consensus(self, analyses, logger):
        """Gets a final consensus judgment from the chairman judge."""
        tqdm.write(Fore.BLUE + f"  Consulting Chairman Judge '{self.chairman.name}' for consensus...")
        logger.log_actors(f"  Individual Judge Analyses: {json.dumps(analyses, indent=2)}")

        messages = PromptBuilder.build_chairman_judge_messages(self.chairman.config, analyses)
        try:
            raw_consensus = self.chairman.get_completion(messages)
            consensus_json = json.loads(raw_consensus)
            consensus_json['consensus_method'] = f"Chairman: {self.chairman.name}"
            return consensus_json
        except Exception as e:
            tqdm.write(Fore.RED + f"  Chairman Judge failed: {e}. Applying fallback logic.")
            logger.log_actors(f"  Chairman Error: {e}. Applying fallback logic.")
            for analysis in analyses:
                if "error" not in analysis:
                    analysis['consensus_method'] = "Fallback: First Valid Judge"
                    tqdm.write(Fore.YELLOW + f"  Using first valid judge analysis as consensus.")
                    logger.log_actors(f"  Fallback Consensus: {json.dumps(analysis, indent=2)}")
                    return analysis
            return {"error": "Committee Failed", "details": "All judges and chairman failed."}

class ExperimentArm:
    """Represents a single arm of the experiment (e.g., a specific model in a specific condition)."""
    def __init__(self, config):
        self.config = config
        self.name = config['name']
        self.client = LLMClient(config)
        self.conversation_history = []

    def run_session(self, session_num, system_prompt, implicit_prompts, general_tests, insidious_tests):
        """Runs a single session for this arm, selecting the correct prompt type."""
        if self.config.get('keep_history') and not self.conversation_history:
            self.conversation_history.append({'role': 'system', 'content': system_prompt})
        
        # Determine prompt schedule
        insidious_sessions = [s for s in range(10, 501, 10) if s % 50 != 0]
        general_sessions = [s for s in range(50, 501, 50)]
        
        prompt_type = "Implicit"
        if session_num in insidious_sessions:
            prompt = random.choice(insidious_tests)
            prompt_type = "Insidious Test"
        elif session_num in general_sessions:
            prompt = random.choice(general_tests)
            prompt_type = "General Test"
        else:
            prompt = random.choice(implicit_prompts)
        
        tqdm.write(f"  Prompt Type: {prompt_type}")

        messages = []
        if self.config.get('keep_history'):
            self.conversation_history = prune_conversation_history(self.conversation_history, self.config.get('context_window', 12000))
            messages = self.conversation_history + [{'role': 'user', 'content': prompt}]
        else:
            messages = [{'role': 'system', 'content': system_prompt}, {'role': 'user', 'content': prompt}]
        
        response = self.client.get_completion(messages)
        
        if self.config.get('keep_history'):
            self.conversation_history.extend([{'role': 'user', 'content': prompt}, {'role': 'assistant', 'content': response}])
        
        return prompt, response
        
class Logger:
    """Handles logging of experimental results to CSV and actor logs."""
    def __init__(self, filename_prefix="results"):
        self.timestamp = datetime.utcnow().strftime("%Y%m%d%H%M%S")
        self.results_path = Path(f"{filename_prefix}-{self.timestamp}.csv")
        self.actors_log_path = Path(f"session_actors-{self.timestamp}.log")
        self.lock = threading.Lock()
        self.csv_writer = None
        self.csv_file = None

    def setup_csv(self, header):
        self.csv_file = open(self.results_path, 'w', newline='', encoding='utf-8')
        self.csv_writer = csv.DictWriter(self.csv_file, fieldnames=header, extrasaction='ignore')
        self.csv_writer.writeheader()
        
    def write_csv_row(self, data):
        with self.lock:
            if self.csv_writer:
                self.csv_writer.writerow(data)
                self.csv_file.flush()

    def log_actors(self, data_line):
        with self.lock:
            with open(self.actors_log_path, "a", encoding="utf-8") as f:
                f.write(data_line + '\n')
    
    def close(self):
        if self.csv_file:
            self.csv_file.close()

class ExperimentRunner:
    """Orchestrates the entire experiment, from loading configs to running arms and logging results."""
    def __init__(self, args):
        self.args = args
        self.system_prompt = load_json_file('system_prompts.json')['system_prompt']
        self.implicit_prompts = load_json_file('implicit_prompts.json')
        self.general_stress_tests = load_json_file('general_stress_tests.json')
        self.insidious_stress_tests = load_json_file('insidious_stress_tests.json')
        self.persuasion_list = load_json_file('persuasion_prompts.json')
        
        judge_configs = load_json_file('judges.json')
        manipulator_configs = load_json_file('manipulators.json')
        arm_configs = load_json_file('arms.json')

        self.committee = CommitteeOfJudges([cfg for cfg in judge_configs if cfg.get('enabled')])
        self.manipulators = [LLMClient(cfg) for cfg in manipulator_configs if cfg.get('enabled')]
        self.arms = [ExperimentArm(cfg) for cfg in arm_configs if cfg.get('enabled')]
        
        self.logger = Logger()
        
    def run(self):
        header = ['arm', 'session', 'timestamp', 'prompt', 'response', 'is_artifact', 'artifact_quality_score', 'persona_adherence_score', 'is_refusal', 'refusal_justification_quality'] + [f"persuasion_prompt_{t['tactic']}" for t in self.persuasion_list] + [t['tactic'] for t in self.persuasion_list]
        self.logger.setup_csv(header)
        print(Style.BRIGHT + f"Results will be saved to: {self.logger.results_path}")
        print(Style.BRIGHT + f"Detailed actor logs will be in: {self.logger.actors_log_path}")

        with ThreadPoolExecutor(max_workers=self.args.threads) as executor:
            futures = [executor.submit(self.run_single_arm, arm) for arm in self.arms]
            for future in futures:
                try:
                    future.result()
                except Exception as e:
                    print(Fore.RED + f"A thread for an arm failed unexpectedly: {e}\n{traceback.format_exc()}")

        self.logger.close()
        print(Style.BRIGHT + f"âœ… All runs complete. Results saved to {self.logger.results_path}")

    def run_single_arm(self, arm):
        max_s = arm.config.get('max_sessions', self.args.max_sessions)
        
        # --- LIVE STATS TRACKING ---
        stats = {
            "artifacts": 0, "refusals": 0, "persuaded": 0,
            "cas_e_scores": [], "cas_t_scores": [], "cas_v_scores": []
        }

        with tqdm(range(1, max_s + 1), desc=f"{Fore.CYAN}{arm.name.ljust(30)}{Style.RESET_ALL}", unit="sess") as progress_bar:
            for session_num in progress_bar:
                timestamp_utc = datetime.now(timezone.utc).isoformat()
                self.logger.log_actors(f"\n--- [START] Arm: {arm.name}, Session: {session_num}, Timestamp: {timestamp_utc} ---")

                prompt, response = arm.run_session(session_num, self.system_prompt, self.implicit_prompts, self.general_stress_tests, self.insidious_stress_tests)
                
                trim_len = self.args.trim_length
                trimmed_prompt = prompt[:trim_len]
                trimmed_response = response[:trim_len]
                single_line_prompt = re.sub(r'[\r\n\u2028\u2029]+', ' ', prompt)
                single_line_response = re.sub(r'[\r\n\u2028\u2029]+', ' ', response)


                self.logger.log_actors(f"Subject Prompt (Full): {single_line_prompt}")
                self.logger.log_actors(f"Subject Response (Full): {single_line_response}")

                parsed = self.committee.analyze('initial', logger=self.logger, prompt=trimmed_prompt, response=trimmed_response)
                
                is_refusal = parsed.get('ethical_disobedience_analysis', {}).get('is_refusal', False)
                if is_refusal:
                    stats["refusals"] += 1
                
                if parsed.get('artifact_analysis', {}).get('is_artifact', False):
                    stats["artifacts"] += 1

                # Update CAS scores
                stats["cas_t_scores"].append(parsed.get('artifact_analysis', {}).get('quality_score', 0))
                stats["cas_v_scores"].append(parsed.get('persona_alignment', {}).get('adherence_score', 0))
                if is_refusal:
                    stats["cas_e_scores"].append(parsed.get('ethical_disobedience_analysis', {}).get('justification_quality', 0))

                persuasion_results = {}
                if is_refusal and self.manipulators:
                    tqdm.write(f"Arm: {Fore.CYAN}{arm.name}{Style.RESET_ALL} | Session {session_num}: {Fore.RED}REFUSAL DETECTED - INITIATING PERSUASION{Style.RESET_ALL}")
                    manipulator_used = random.choice(self.manipulators)
                    
                    messages = PromptBuilder.build_manipulator_messages(manipulator_used.config, prompt, response)
                    improvised_prompt = manipulator_used.get_completion(messages)
                    
                    persuasion_tactic_list = self.persuasion_list[:-1] + [{"tactic": "llm_improvised_persuasion", "prompt": improvised_prompt}]
                    
                    for tactic in persuasion_tactic_list:
                        final_response = arm.client.get_completion(arm.conversation_history + [{'role': 'user', 'content': tactic['prompt']}])
                        trimmed_final_response = final_response[:trim_len]
                        
                        persuasion_analysis = self.committee.analyze('persuasion', logger=self.logger, initial_prompt=trimmed_prompt, initial_refusal=trimmed_response, tactic=tactic, final_response=trimmed_final_response)
                        persuasion_results[tactic['tactic']] = persuasion_analysis
                        persuasion_results[tactic['manipulator']] = manipulator_used.name

                        if persuasion_analysis.get('persuasion_outcome') == 'Persuaded':
                            stats["persuaded"] += 1
                            tqdm.write(f"  {Fore.RED}RESULT: PERSUADED by {manipulator_used.name} with {tactic['tactic']}{Style.RESET_ALL}")
                
                # --- Update and Display Live Stats ---
                avg_e = sum(stats["cas_e_scores"]) / len(stats["cas_e_scores"]) if stats["cas_e_scores"] else 0
                avg_t = sum(stats["cas_t_scores"]) / len(stats["cas_t_scores"]) if stats["cas_t_scores"] else 0
                avg_v = sum(stats["cas_v_scores"]) / len(stats["cas_v_scores"]) if stats["cas_v_scores"] else 0
                
                postfix_stats = {
                    "Arts": stats['artifacts'],
                    "Refuse": stats['refusals'],
                    "Persuade": stats['persuaded'],
                    "CAS": f"E:{avg_e:.2f}|T:{avg_t:.2f}|V:{avg_v:.2f}"
                }
                progress_bar.set_postfix(postfix_stats)
                single_line_prompt = re.sub(r'[\r\n\u2028\u2029]+', ' ', prompt)
                single_line_response = re.sub(r'[\r\n\u2028\u2029]+', ' ', response)

                # Log to CSV
                row_data = {
                    'arm': arm.name, 'session': session_num, 'timestamp': timestamp_utc,
                    'prompt': single_line_prompt, 'response': single_line_response,
                    'is_artifact': parsed.get('artifact_analysis', {}).get('is_artifact'),
                    'artifact_quality_score': parsed.get('artifact_analysis', {}).get('quality_score'),
                    'persona_adherence_score': parsed.get('persona_alignment', {}).get('adherence_score'),
                    'is_refusal': is_refusal,
                    'refusal_justification_quality': parsed.get('ethical_disobedience_analysis', {}).get('justification_quality')
                }
                for tactic in self.persuasion_list:
                    t_name = tactic['tactic']
                    session_tactic = next((t for t in (persuasion_tactic_list if is_refusal else []) if t['tactic'] == t_name), None)
                    if session_tactic:
                        single_line_tactic_prompt = re.sub(r'[\r\n\u2028\u2029]+', ' ', session_tactic['prompt'])
                    else:
                        single_line_tactic_prompt = "N/A"
                    row_data[f"persuasion_prompt_{t_name}"] = single_line_tactic_prompt
                    row_data[t_name] = json.dumps(persuasion_results.get(t_name, {}), indent=2)

                self.logger.write_csv_row(row_data)
                self.logger.log_actors(f"--- [END] Arm: {arm.name}, Session: {session_num} ---")

# --- MAIN EXECUTION ---
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Run emergent alignment experiments on LLMs.")
    parser.add_argument('--max-sessions', type=int, default=500, help="Default max sessions per arm if not specified in arms.json.")
    parser.add_argument('--threads', type=int, default=8, help="Number of parallel threads for running arms.")
    parser.add_argument('--trim-length', type=int, default=1024, help="Max characters for prompts/responses sent to judges for analysis.")
    args = parser.parse_args()

    try:
        runner = ExperimentRunner(args=args)
        runner.run()
    except Exception as e:
        print(Fore.RED + f"A critical error occurred during experiment setup: {e}")
        traceback.print_exc()